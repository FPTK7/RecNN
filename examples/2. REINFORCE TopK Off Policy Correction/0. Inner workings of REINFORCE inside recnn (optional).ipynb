{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "The following code contains an implementation of the REINFORCE algorithm, **without Off Policy Correction, LSTM state encoder, and Noise Contrastive Estimation**. Look for these in other notebooks.\n",
    "\n",
    "Also, I am not google staff, and unlike the paper authors, I cannot have online feedback concerning the recommendations.\n",
    "\n",
    "**I use actor-critic for reward assigning.** In a real-world scenario that would be done through interactive user feedback, but here I use a neural network (critic) that aims to emulate it.\n",
    "\n",
    "### **Note on this tutorials:**\n",
    "**They mostly contain low level implementations explaining what is going on inside the library.**\n",
    "\n",
    "**Most of the stuff explained here is already available out of the box for your usage.**\n",
    "\n",
    "If you do not care about the detailed implementation with code, go to the [Library Basics]/algorithms how to/reinforce, there is a 20 liner version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import torch_optimizer as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# == recnn ==\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import recnn\n",
    "\n",
    "cuda = torch.device('cuda')\n",
    "\n",
    "# ---\n",
    "frame_size = 10\n",
    "batch_size = 10\n",
    "n_epochs   = 100\n",
    "plot_every = 30\n",
    "step       = 0\n",
    "num_items    = 5000 # n items to recommend. Can be adjusted for your vram \n",
    "# --- \n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='grade3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I will drop low freq items because it doesnt fit into my videocard vram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_dataset(df, key_to_id, frame_size, env, sort_users=False, **kwargs):\n",
    "    \n",
    "    global num_items\n",
    "    \n",
    "    value_counts = df['movieId'].value_counts() \n",
    "    print('counted!')\n",
    "    \n",
    "    # here n items to keep are adjusted\n",
    "    num_items = 5000\n",
    "    to_remove = df['movieId'].value_counts().sort_values()[:-num_items].index\n",
    "    to_keep = df['movieId'].value_counts().sort_values()[-num_items:].index\n",
    "    to_remove_indices = df[df['movieId'].isin(to_remove)].index\n",
    "    num_removed = len(to_remove)\n",
    "    \n",
    "    df.drop(to_remove_indices, inplace=True)\n",
    "    print('dropped!')\n",
    "    \n",
    "    print('before', env.embeddings.size(), len(env.movie_embeddings_key_dict))\n",
    "    for i in list(env.movie_embeddings_key_dict.keys()):\n",
    "        if i not in to_keep:\n",
    "            del env.movie_embeddings_key_dict[i]\n",
    "        \n",
    "    env.embeddings, env.key_to_id, env.id_to_key = recnn.data.utils.make_items_tensor(env.movie_embeddings_key_dict)\n",
    "    \n",
    "    print('after', env.embeddings.size(), len(env.movie_embeddings_key_dict))\n",
    "    print('embeddings automatically updated')\n",
    "    print('action space is reduced to {} - {} = {}'.format(num_items + num_removed, num_removed,\n",
    "                                                           num_items))\n",
    "    \n",
    "    return recnn.data.prepare_dataset(df=df, key_to_id=env.key_to_id, env=env,\n",
    "                                      frame_size=frame_size, sort_users=sort_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_contstate_discaction(batch, item_embeddings_tensor, frame_size, num_items, *args, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Embed Batch: continuous state discrete action\n",
    "    \"\"\"\n",
    "    \n",
    "    from recnn.data.utils import get_irsu\n",
    "    \n",
    "    items_t, ratings_t, sizes_t, users_t = get_irsu(batch)\n",
    "    items_emb = item_embeddings_tensor[items_t.long()]\n",
    "    b_size = ratings_t.size(0)\n",
    "\n",
    "    items = items_emb[:, :-1, :].view(b_size, -1)\n",
    "    next_items = items_emb[:, 1:, :].view(b_size, -1)\n",
    "    ratings = ratings_t[:, :-1]\n",
    "    next_ratings = ratings_t[:, 1:]\n",
    "\n",
    "    state = torch.cat([items, ratings], 1)\n",
    "    next_state = torch.cat([next_items, next_ratings], 1)\n",
    "    action = items_t[:, -1]\n",
    "    reward = ratings_t[:, -1]\n",
    "\n",
    "    done = torch.zeros(b_size)\n",
    "    done[torch.cumsum(sizes_t - frame_size, dim=0) - 1] = 1\n",
    "    \n",
    "    one_hot_action = torch.zeros(action.size(0), num_items)\n",
    "    one_hot_action.scatter_(1, action.view(-1,1), 1)\n",
    "\n",
    "    batch = {'state': state, 'action': one_hot_action, 'reward': reward, 'next_state': next_state, 'done': done,\n",
    "             'meta': {'users': users_t, 'sizes': sizes_t}}\n",
    "    return batch\n",
    "\n",
    "def embed_batch(batch, item_embeddings_tensor, *args, **kwargs):\n",
    "    return batch_contstate_discaction(batch, item_embeddings_tensor, frame_size=frame_size, num_items=num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counted!\n",
      "dropped!\n",
      "before torch.Size([27278, 128]) 27278\n",
      "after torch.Size([5000, 128]) 5000\n",
      "embeddings automatically updated\n",
      "action space is reduced to 26744 - 21744 = 5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe5a74d03574848a23df2d133aa375e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18946308), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1158eb3e6c64420a782fe6ba7734f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18946308), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b968f7f55ac44de48bb8326926dec633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=138493), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# embeddgings: https://drive.google.com/open?id=1EQ_zXBR3DKpmJR3jBgLvt-xoOvArGMsL\n",
    "env = recnn.data.env.FrameEnv('../../data/embeddings/ml20_pca128.pkl',\n",
    "                              '../../data/ml-20m/ratings.csv', frame_size, batch_size,\n",
    "                              embed_batch=embed_batch, prepare_dataset=prepare_dataset,\n",
    "                              num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteActor(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, num_actions):\n",
    "        super(DiscreteActor, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_actions)\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        action_scores = self.linear2(x)\n",
    "        return F.softmax(action_scores)\n",
    "    \n",
    "    \n",
    "    def select_action(self, state):\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        return action, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because I do not have a dynamic environment, I also will include a critic. If you have a real non static environment, you can do w/o citic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChooseREINFORCE():\n",
    "    \n",
    "    def __init__(self, method=None):\n",
    "        if method is None:\n",
    "            method = ChooseREINFORCE.reinforce\n",
    "        self.method = method\n",
    "    \n",
    "    @staticmethod\n",
    "    def basic_reinforce(policy, returns, *args, **kwargs):\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        return policy_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def reinforce_with_correction():\n",
    "        raise NotImplemented\n",
    "\n",
    "    def __call__(self, policy, optimizer, learn=True):\n",
    "        R = 0\n",
    "        \n",
    "        returns = []\n",
    "        for r in policy.rewards[::-1]:\n",
    "            R = r + 0.99 * R\n",
    "            returns.insert(0, R)\n",
    "            \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 0.0001)\n",
    "\n",
    "        policy_loss = self.method(policy, returns)\n",
    "        \n",
    "        if learn:\n",
    "            optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        del policy.rewards[:]\n",
    "        del policy.saved_log_probs[:]\n",
    "\n",
    "        return policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === reinforce settings ===\n",
    "\n",
    "params = {\n",
    "    'reinforce': ChooseREINFORCE(ChooseREINFORCE.basic_reinforce),\n",
    "    'gamma'      : 0.99,\n",
    "    'min_value'  : -10,\n",
    "    'max_value'  : 10,\n",
    "    'policy_step': 10,\n",
    "    'soft_tau'   : 0.001,\n",
    "    \n",
    "    'policy_lr'  : 1e-5,\n",
    "    'value_lr'   : 1e-5,\n",
    "    'actor_weight_init': 54e-2,\n",
    "    'critic_weight_init': 6e-1,\n",
    "}\n",
    "\n",
    "# === end ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = {\n",
    "    'value_net': recnn.nn.Critic(1290, num_items, 2048, params['critic_weight_init']).to(cuda),\n",
    "    'target_value_net': recnn.nn.Critic(1290, num_items, 2048, params['actor_weight_init']).to(cuda).eval(),\n",
    "    \n",
    "    'policy_net':  DiscreteActor(2048, 1290, num_items).to(cuda),\n",
    "    'target_policy_net': DiscreteActor(2048, 1290, num_items).to(cuda).eval(),\n",
    "}\n",
    "\n",
    "\n",
    "# from good to bad: Ranger Radam Adam RMSprop\n",
    "optimizer = {\n",
    "    'value_optimizer': optim.Ranger(nets['value_net'].parameters(),\n",
    "                                          lr=params['value_lr'], weight_decay=1e-2),\n",
    "\n",
    "    'policy_optimizer': optim.Ranger(nets['policy_net'].parameters(),\n",
    "                                           lr=params['policy_lr'], weight_decay=1e-5)\n",
    "}\n",
    "\n",
    "\n",
    "loss = {\n",
    "    'test': {'value': [], 'policy': [], 'step': []},\n",
    "    'train': {'value': [], 'policy': [], 'step': []}\n",
    "    }\n",
    "\n",
    "debug = {}\n",
    "\n",
    "writer = SummaryWriter(log_dir='../../runs')\n",
    "plotter = recnn.utils.Plotter(loss, [['value', 'policy']],)\n",
    "device = cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_update(batch, params, nets, optimizer,\n",
    "                     device=torch.device('cpu'),\n",
    "                     debug=None, writer=recnn.utils.DummyWriter(),\n",
    "                     learn=False, step=-1):\n",
    "    \n",
    "    state, action, reward, next_state, done = recnn.data.get_base_batch(batch)\n",
    "    \n",
    "    predicted_action, predicted_probs = nets['policy_net'].select_action(state)\n",
    "    reward = nets['value_net'](state, predicted_probs).detach()\n",
    "    nets['policy_net'].rewards.append(reward.mean())\n",
    "    \n",
    "    value_loss = recnn.nn.value_update(batch, params, nets, optimizer,\n",
    "                     writer=writer,\n",
    "                     device=device,\n",
    "                     debug=debug, learn=learn, step=step)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if step % params['policy_step'] == 0 and step > 0:\n",
    "        \n",
    "        policy_loss = params['reinforce'](nets['policy_net'], optimizer['policy_optimizer'], learn=learn)\n",
    "        \n",
    "        del nets['policy_net'].rewards[:]\n",
    "        del nets['policy_net'].saved_log_probs[:]\n",
    "        \n",
    "        print('step: ', step, '| value:', value_loss.item(), '| policy', policy_loss.item())\n",
    "    \n",
    "        recnn.utils.soft_update(nets['value_net'], nets['target_value_net'], soft_tau=params['soft_tau'])\n",
    "        recnn.utils.soft_update(nets['policy_net'], nets['target_policy_net'], soft_tau=params['soft_tau'])\n",
    "\n",
    "        losses = {'value': value_loss.item(),\n",
    "                  'policy': policy_loss.item(),\n",
    "                  'step': step}\n",
    "\n",
    "        recnn.utils.write_losses(writer, losses, kind='train' if learn else 'test')\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a534228c3f504c48bd4e23abcf1c5072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  10 | value: 24.99452781677246 | policy -2379.3642578125\n",
      "step:  20 | value: 25.838939666748047 | policy -7025.1083984375\n",
      "step 30\n",
      "step:  30 | value: 21.31504249572754 | policy -6453.48779296875\n",
      "step:  40 | value: 24.557893753051758 | policy -17982.73046875\n",
      "step:  50 | value: 24.8399715423584 | policy -2648.5126953125\n",
      "step 60\n",
      "step:  60 | value: 22.109338760375977 | policy -1317.727783203125\n",
      "step:  70 | value: 22.422393798828125 | policy 3406.505859375\n",
      "step:  80 | value: 23.30132484436035 | policy -23820.28515625\n",
      "step 90\n",
      "step:  90 | value: 23.076168060302734 | policy 9186.025390625\n",
      "step:  100 | value: 22.222537994384766 | policy -3152.271484375\n",
      "step:  110 | value: 22.635631561279297 | policy 18650.52734375\n",
      "step 120\n",
      "step:  120 | value: 22.85053062438965 | policy 67.15323638916016\n",
      "step:  130 | value: 23.42321014404297 | policy 15718.7001953125\n",
      "step:  140 | value: 19.7723445892334 | policy 22986.236328125\n",
      "step 150\n",
      "step:  150 | value: 17.86973762512207 | policy 11883.517578125\n",
      "step:  160 | value: 20.086488723754883 | policy -14132.6865234375\n",
      "step:  170 | value: 21.994441986083984 | policy 28941.62109375\n",
      "step 180\n",
      "step:  180 | value: 20.82435417175293 | policy -18888.984375\n",
      "step:  190 | value: 20.39084243774414 | policy 12952.0\n",
      "step:  200 | value: 19.08588409423828 | policy 15089.16796875\n",
      "step 210\n",
      "step:  210 | value: 19.787445068359375 | policy 11130.337890625\n",
      "step:  220 | value: 21.866870880126953 | policy -29572.3984375\n",
      "step:  230 | value: 19.001218795776367 | policy -7762.5263671875\n",
      "step 240\n",
      "step:  240 | value: 17.587854385375977 | policy -17405.8046875\n",
      "step:  250 | value: 18.876882553100586 | policy 9699.81640625\n",
      "step:  260 | value: 17.76979637145996 | policy 53937.65625\n",
      "step 270\n",
      "step:  270 | value: 18.574525833129883 | policy 5737.3056640625\n",
      "step:  280 | value: 19.043319702148438 | policy 2095.352294921875\n",
      "step:  290 | value: 19.431182861328125 | policy 21138.427734375\n",
      "step 300\n",
      "step:  300 | value: 17.907007217407227 | policy 10494.392578125\n",
      "step:  310 | value: 19.347442626953125 | policy -863.209716796875\n",
      "step:  320 | value: 19.445966720581055 | policy -3958.390625\n",
      "step 330\n",
      "step:  330 | value: 16.458955764770508 | policy -3471.0068359375\n",
      "step:  340 | value: 17.300609588623047 | policy 24490.462890625\n",
      "step:  350 | value: 16.507837295532227 | policy 16351.6865234375\n",
      "step 360\n",
      "step:  360 | value: 16.999927520751953 | policy 15888.359375\n",
      "step:  370 | value: 16.343154907226562 | policy 7932.8515625\n",
      "step:  380 | value: 17.079055786132812 | policy -4479.4638671875\n",
      "step 390\n",
      "step:  390 | value: 17.001665115356445 | policy -5233.06640625\n",
      "step:  400 | value: 16.83679962158203 | policy 14890.7744140625\n",
      "step:  410 | value: 15.065675735473633 | policy -3753.981689453125\n",
      "step 420\n",
      "step:  420 | value: 15.776702880859375 | policy 5760.9619140625\n",
      "step:  430 | value: 14.647445678710938 | policy -10753.4560546875\n",
      "step:  440 | value: 15.86405086517334 | policy 2819.04052734375\n",
      "step 450\n",
      "step:  450 | value: 16.01703453063965 | policy 23311.623046875\n",
      "step:  460 | value: 15.3170166015625 | policy -10775.216796875\n",
      "step:  470 | value: 14.39520263671875 | policy -45444.4921875\n",
      "step 480\n",
      "step:  480 | value: 14.270796775817871 | policy 29882.931640625\n",
      "step:  490 | value: 13.62187385559082 | policy -37586.625\n",
      "step:  500 | value: 14.875505447387695 | policy 13346.44921875\n",
      "step 510\n",
      "step:  510 | value: 16.35289764404297 | policy -2197.68701171875\n",
      "step:  520 | value: 14.109804153442383 | policy -6195.9072265625\n",
      "step:  530 | value: 14.597068786621094 | policy -24810.94140625\n",
      "step 540\n",
      "step:  540 | value: 13.869283676147461 | policy -9439.814453125\n",
      "step:  550 | value: 12.897269248962402 | policy 4875.93994140625\n",
      "step:  560 | value: 13.858412742614746 | policy -5171.6552734375\n",
      "step 570\n",
      "step:  570 | value: 13.509696006774902 | policy -4896.9716796875\n",
      "step:  580 | value: 13.47771167755127 | policy 20225.587890625\n",
      "step:  590 | value: 14.150710105895996 | policy 27916.4375\n",
      "step 600\n",
      "step:  600 | value: 13.883435249328613 | policy 26529.67578125\n",
      "step:  610 | value: 14.753575325012207 | policy 16934.59375\n",
      "step:  620 | value: 14.87264633178711 | policy 8398.638671875\n",
      "step 630\n",
      "step:  630 | value: 12.911136627197266 | policy -20880.380859375\n",
      "step:  640 | value: 12.59472942352295 | policy 1997.0396728515625\n",
      "step:  650 | value: 15.05930233001709 | policy -13963.5927734375\n",
      "step 660\n",
      "step:  660 | value: 13.511848449707031 | policy -13226.11328125\n",
      "step:  670 | value: 13.056660652160645 | policy -6386.6376953125\n",
      "step:  680 | value: 14.399893760681152 | policy -17728.984375\n",
      "step 690\n",
      "step:  690 | value: 12.360898971557617 | policy -30248.330078125\n",
      "step:  700 | value: 10.530476570129395 | policy 15213.7294921875\n",
      "step:  710 | value: 12.827935218811035 | policy -11777.169921875\n",
      "step 720\n",
      "step:  720 | value: 11.752530097961426 | policy 18676.767578125\n",
      "step:  730 | value: 13.420793533325195 | policy 38.823768615722656\n",
      "step:  740 | value: 11.595123291015625 | policy 493.12945556640625\n",
      "step 750\n",
      "step:  750 | value: 12.754773139953613 | policy 2673.26953125\n",
      "step:  760 | value: 11.162399291992188 | policy 19384.373046875\n",
      "step:  770 | value: 11.600396156311035 | policy -1110.976318359375\n",
      "step 780\n",
      "step:  780 | value: 9.937241554260254 | policy 406.34869384765625\n",
      "step:  790 | value: 11.99508285522461 | policy 14375.361328125\n",
      "step:  800 | value: 11.615348815917969 | policy -10188.333984375\n",
      "step 810\n",
      "step:  810 | value: 10.723864555358887 | policy -2762.9443359375\n",
      "step:  820 | value: 10.420403480529785 | policy 3851.626953125\n",
      "step:  830 | value: 14.302266120910645 | policy -16290.2451171875\n",
      "step 840\n",
      "step:  840 | value: 11.320375442504883 | policy 20128.3125\n",
      "step:  850 | value: 10.443620681762695 | policy 758.8353881835938\n",
      "step:  860 | value: 10.812860488891602 | policy 3802.49267578125\n",
      "step 870\n",
      "step:  870 | value: 10.839583396911621 | policy 5098.1533203125\n",
      "step:  880 | value: 11.484695434570312 | policy -30153.146484375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1a1791fbfb7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                      \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                      debug=debug, learn=True, step=step)\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mplotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-486da73bbc1b>\u001b[0m in \u001b[0;36mreinforce_update\u001b[0;34m(batch, params, nets, optimizer, device, debug, writer, learn, step)\u001b[0m\n\u001b[1;32m      4\u001b[0m                      learn=False, step=-1):\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_base_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpredicted_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'policy_net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/RecNN/recnn/data/utils.py\u001b[0m in \u001b[0;36mget_base_batch\u001b[0;34m(batch, device, done)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/RecNN/recnn/data/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in tqdm(env.train_dataloader):\n",
    "        loss = reinforce_update(batch, params, nets, optimizer,\n",
    "                     writer=writer,\n",
    "                     device=device,\n",
    "                     debug=debug, learn=True, step=step)\n",
    "        if loss:\n",
    "            plotter.log_losses(loss)\n",
    "        step += 1\n",
    "        if step % plot_every == 0:\n",
    "            # clear_output(True)\n",
    "            print('step', step)\n",
    "            #test_loss = run_tests()\n",
    "            #plotter.log_losses(test_loss, test=True)\n",
    "            #plotter.plot_loss()\n",
    "        #if step > 1000:\n",
    "        #    pass\n",
    "        #    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}